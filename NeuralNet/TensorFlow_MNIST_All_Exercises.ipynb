{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "There are several main adjustments you may try.\n",
    "\n",
    "Please pay attention to the time it takes for each epoch to conclude.\n",
    "\n",
    "Using the code from the lecture as the basis, fiddle with the hyperparameters of the algorithm.\n",
    "\n",
    "1. The *width* (the hidden layer size) of the algorithm. Try a hidden layer size of 200. How does the validation accuracy of the model change? What about the time it took the algorithm to train? Can you find a hidden layer size that does better?\n",
    "\n",
    "2. The *depth* of the algorithm. Add another hidden layer to the algorithm. This is an extremely important exercise! How does the validation accuracy change? What about the time it took the algorithm to train? Hint: Be careful with the shapes of the weights and the biases.\n",
    "\n",
    "3. The *width and depth* of the algorithm. Add as many additional layers as you need to reach 5 hidden layers. Moreover, adjust the width of the algorithm as you find suitable. How does the validation accuracy change? What about the time it took the algorithm to train?\n",
    "\n",
    "4. Fiddle with the activation functions. Try applying sigmoid transformation to both layers. The sigmoid activation is given by the string 'sigmoid'.\n",
    "\n",
    "5. Fiddle with the activation functions. Try applying a ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the string 'tanh'.\n",
    "\n",
    "6. Adjust the batch size. Try a batch size of 10000. How does the required time change? What about the accuracy?\n",
    "\n",
    "7. Adjust the batch size. Try a batch size of 1. That's the SGD. How do the time and accuracy change? Is the result coherent with the theory?\n",
    "\n",
    "8. Adjust the learning rate. Try a value of 0.0001. Does it make a difference?\n",
    "\n",
    "9. Adjust the learning rate. Try a value of 0.02. Does it make a difference?\n",
    "\n",
    "10. Combine all the methods above and try to reach a validation accuracy of 98.5+ percent.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" of deep learning because for most students it is the first deep learning algorithm they see.\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs). \n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "That's where we load and preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I am changing dataDir of models\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True, data_dir=\"F:\\Reference\\AI\\completeDsUdemy\\dataDir\")\n",
    "# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
    "# as_supervised=True -> (input, target) \n",
    "\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "\n",
    "# let's also shuffle the data\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets\n",
    "# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling\n",
    "\n",
    "# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# determine the batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# we can also take advantage of the occasion to batch the train data\n",
    "# this would be very helpful when we train, as we would be able to iterate over the different batches\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the model\n",
    "When thinking about a deep learning algorithm, we mostly imagine building the model. So, let's do it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 50\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    # the first layer (the input layer)\n",
    "    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
    "    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images\n",
    "    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) \n",
    "    # or (28x28x1,) = (784,) vector\n",
    "    # this allows us to actually create a feed forward neural network\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    \n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "That's where we train the model we have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 1s - loss: 0.4089 - accuracy: 0.8838 - val_loss: 0.2055 - val_accuracy: 0.9388\n",
      "Epoch 2/5\n",
      "540/540 - 1s - loss: 0.1805 - accuracy: 0.9474 - val_loss: 0.1417 - val_accuracy: 0.9563\n",
      "Epoch 3/5\n",
      "540/540 - 1s - loss: 0.1385 - accuracy: 0.9586 - val_loss: 0.1127 - val_accuracy: 0.9652\n",
      "Epoch 4/5\n",
      "540/540 - 1s - loss: 0.1122 - accuracy: 0.9660 - val_loss: 0.0945 - val_accuracy: 0.9713\n",
      "Epoch 5/5\n",
      "540/540 - 1s - loss: 0.0931 - accuracy: 0.9719 - val_loss: 0.0907 - val_accuracy: 0.9708\n",
      "--- 5.476787090301514 seconds --- for 50 size\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def train():\n",
    "    start_time = time.time()\n",
    "    NUM_EPOCHS = 5\n",
    "    model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)\n",
    "    print(f\"--- %s seconds --- for {hidden_layer_size} size\" % (time.time() - start_time))\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a hidden layer size of 200. \n",
    " How does the validation accuracy of the model change? What about the time it took the algorithm to train? Can you find a hidden layer size that does better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 6s - loss: 0.2769 - accuracy: 0.9196 - val_loss: 0.1303 - val_accuracy: 0.9603\n",
      "Epoch 2/5\n",
      "540/540 - 4s - loss: 0.1033 - accuracy: 0.9684 - val_loss: 0.0871 - val_accuracy: 0.9727\n",
      "Epoch 3/5\n",
      "540/540 - 4s - loss: 0.0703 - accuracy: 0.9786 - val_loss: 0.0648 - val_accuracy: 0.9813\n",
      "Epoch 4/5\n",
      "540/540 - 4s - loss: 0.0517 - accuracy: 0.9839 - val_loss: 0.0513 - val_accuracy: 0.9843\n",
      "Epoch 5/5\n",
      "540/540 - 4s - loss: 0.0399 - accuracy: 0.9873 - val_loss: 0.0433 - val_accuracy: 0.9862\n",
      "--- 23.7259578704834 seconds --- for 200 size\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 200\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') \n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The depth of the algorithm. Add another hidden layer to the algorithm.\n",
    "This is an extremely important exercise! How does the validation accuracy change? What about the time it took the algorithm to train? Hint: Be careful with the shapes of the weights and the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 2s - loss: 0.2512 - accuracy: 0.9257 - val_loss: 0.1185 - val_accuracy: 0.9665\n",
      "Epoch 2/5\n",
      "540/540 - 2s - loss: 0.0967 - accuracy: 0.9700 - val_loss: 0.0827 - val_accuracy: 0.9745\n",
      "Epoch 3/5\n",
      "540/540 - 2s - loss: 0.0677 - accuracy: 0.9792 - val_loss: 0.0599 - val_accuracy: 0.9812\n",
      "Epoch 4/5\n",
      "540/540 - 2s - loss: 0.0497 - accuracy: 0.9840 - val_loss: 0.0518 - val_accuracy: 0.9852\n",
      "Epoch 5/5\n",
      "540/540 - 2s - loss: 0.0398 - accuracy: 0.9871 - val_loss: 0.0399 - val_accuracy: 0.9883\n",
      "--- 8.005591630935669 seconds --- for 200 size\n",
      "with extra elu layer\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 200\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='elu'), \n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') \n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "train()\n",
    "print(\"with extra elu layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 hidden layers and Fiddle with the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 8s - loss: 0.3155 - accuracy: 0.9057 - val_loss: 0.1343 - val_accuracy: 0.9577\n",
      "Epoch 2/5\n",
      "540/540 - 6s - loss: 0.1162 - accuracy: 0.9644 - val_loss: 0.1246 - val_accuracy: 0.9617\n",
      "Epoch 3/5\n",
      "540/540 - 6s - loss: 0.0824 - accuracy: 0.9748 - val_loss: 0.0800 - val_accuracy: 0.9773\n",
      "Epoch 4/5\n",
      "540/540 - 6s - loss: 0.0639 - accuracy: 0.9805 - val_loss: 0.0654 - val_accuracy: 0.9808\n",
      "Epoch 5/5\n",
      "540/540 - 6s - loss: 0.0498 - accuracy: 0.9843 - val_loss: 0.0589 - val_accuracy: 0.9832\n",
      "--- 32.08390545845032 seconds --- for 200 size\n",
      "5 hidden layers\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 200\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='elu'), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='PReLU'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='sigmoid'), \n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') \n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "train()\n",
    "print(\"5 hidden layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 973ms/step - loss: 0.0888 - accuracy: 0.9745\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try applying a ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the string 'tanh'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 6s - loss: 0.4017 - accuracy: 0.8917 - val_loss: 0.1897 - val_accuracy: 0.9457\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.1687 - accuracy: 0.9515 - val_loss: 0.1279 - val_accuracy: 0.9622\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.1217 - accuracy: 0.9645 - val_loss: 0.0985 - val_accuracy: 0.9725\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0974 - accuracy: 0.9715 - val_loss: 0.0835 - val_accuracy: 0.9762\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0802 - accuracy: 0.9759 - val_loss: 0.0770 - val_accuracy: 0.9777\n",
      "--- 17.147287607192993 seconds --- for 50 size\n",
      "relu tanh as hidden layers\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='tanh'), \n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') \n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "train()\n",
    "print(\"relu tanh as hidden layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the batch size. \n",
    "Try a batch size of 10000. How does the required time change? What about the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6/6 - 5s - loss: 2.1650 - accuracy: 0.2618 - val_loss: 1.8955 - val_accuracy: 0.5427\n",
      "Epoch 2/5\n",
      "6/6 - 2s - loss: 1.7520 - accuracy: 0.5952 - val_loss: 1.5164 - val_accuracy: 0.6685\n",
      "Epoch 3/5\n",
      "6/6 - 2s - loss: 1.3940 - accuracy: 0.6937 - val_loss: 1.2035 - val_accuracy: 0.7363\n",
      "Epoch 4/5\n",
      "6/6 - 2s - loss: 1.1106 - accuracy: 0.7545 - val_loss: 0.9681 - val_accuracy: 0.7837\n",
      "Epoch 5/5\n",
      "6/6 - 2s - loss: 0.8998 - accuracy: 0.7950 - val_loss: 0.7958 - val_accuracy: 0.8152\n",
      "--- 12.377770900726318 seconds --- for 50 size\n",
      "Try a batch size of 10000 \n"
     ]
    }
   ],
   "source": [
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "# determine the batch size\n",
    "BATCH_SIZE = 10000\n",
    "\n",
    "# we can also take advantage of the occasion to batch the train data\n",
    "# this would be very helpful when we train, as we would be able to iterate over the different batches\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='tanh'), \n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') \n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "train() \n",
    "print(\"Try a batch size of 10000 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the batch size. Try a batch size of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "54000/54000 - 79s - loss: 0.2350 - accuracy: 0.9285 - val_loss: 0.1338 - val_accuracy: 0.9618\n",
      "Epoch 2/5\n",
      "54000/54000 - 78s - loss: 0.1393 - accuracy: 0.9587 - val_loss: 0.1116 - val_accuracy: 0.9690\n",
      "Epoch 3/5\n",
      "54000/54000 - 77s - loss: 0.1136 - accuracy: 0.9663 - val_loss: 0.1137 - val_accuracy: 0.9670\n",
      "Epoch 4/5\n",
      "54000/54000 - 77s - loss: 0.1048 - accuracy: 0.9699 - val_loss: 0.1020 - val_accuracy: 0.9738\n",
      "Epoch 5/5\n",
      "54000/54000 - 77s - loss: 0.0947 - accuracy: 0.9722 - val_loss: 0.0965 - val_accuracy: 0.9730\n",
      "--- 387.3800530433655 seconds --- for 50 size\n",
      "Try a batch size of 1 \n"
     ]
    }
   ],
   "source": [
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "# determine the batch size\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# we can also take advantage of the occasion to batch the train data\n",
    "# this would be very helpful when we train, as we would be able to iterate over the different batches\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='tanh'), \n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') \n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "train() \n",
    "print(\"Try a batch size of 1 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the learning rate. \n",
    "Try a value of 0.0001. Does it make a difference?\n",
    "\n",
    "Adjust the learning rate. Try a value of 0.02. Does it make a difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Try Adam with LR 0.02----------------\n",
      "Epoch 1/5\n",
      "540/540 - 5s - loss: 0.3109 - accuracy: 0.9108 - val_loss: 0.2087 - val_accuracy: 0.9422\n",
      "Epoch 2/5\n",
      "540/540 - 4s - loss: 0.1966 - accuracy: 0.9468 - val_loss: 0.1758 - val_accuracy: 0.9508\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.1937 - accuracy: 0.9485 - val_loss: 0.1841 - val_accuracy: 0.9487\n",
      "Epoch 4/5\n",
      "540/540 - 4s - loss: 0.1696 - accuracy: 0.9558 - val_loss: 0.1501 - val_accuracy: 0.9623\n",
      "Epoch 5/5\n",
      "540/540 - 4s - loss: 0.1531 - accuracy: 0.9606 - val_loss: 0.1393 - val_accuracy: 0.9645\n",
      "--- 18.88952875137329 seconds --- for 200 size\n",
      "--------------------Try Adam with LR 0.0001----------------\n",
      "Epoch 1/5\n",
      "540/540 - 4s - loss: 0.1123 - accuracy: 0.9698 - val_loss: 0.1124 - val_accuracy: 0.9705\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.0960 - accuracy: 0.9741 - val_loss: 0.1045 - val_accuracy: 0.9732\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.0893 - accuracy: 0.9750 - val_loss: 0.0997 - val_accuracy: 0.9733\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0845 - accuracy: 0.9760 - val_loss: 0.0956 - val_accuracy: 0.9735\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0794 - accuracy: 0.9773 - val_loss: 0.0926 - val_accuracy: 0.9737\n",
      "--- 17.81906819343567 seconds --- for 200 size\n"
     ]
    }
   ],
   "source": [
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# determine the batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# we can also take advantage of the occasion to batch the train data\n",
    "# this would be very helpful when we train, as we would be able to iterate over the different batches\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "#-----------------------------------\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 200\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') \n",
    "])\n",
    "print(\"--------------------Try Adam with LR 0.02----------------\")\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.02)\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "train()\n",
    "print(\"--------------------Try Adam with LR 0.0001----------------\")\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model - below is for original 2 layers and 50 of them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 658ms/step - loss: 0.1079 - accuracy: 0.9681\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.11. Test accuracy: 96.81%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
